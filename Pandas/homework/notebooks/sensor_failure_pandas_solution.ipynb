{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdf1426",
   "metadata": {},
   "source": [
    "# Enunciado\n",
    "Hola ingeniero 👋. Bienvenido a tu sexto desafio! Ya sabes que en [Vault-Tec Corporation](https://fallout.fandom.com/es/wiki/Vault-Tec_Corporation) tenemos sensores encargados de monitorear las condiciones actuales de nuestros refugios. En ocasiones pasadas detectamos que varios de ellos presentaban fallas recurrentes, impidiendo que pudieramos determinar las condiciones de nuestros bóvedas y darle la seguridad esperada a nuestros clientes en un mundo post apocaliptico 🙃. \n",
    "\n",
    "Anteriormente, a pedido de nuestro equipo, solucionamos algunos errores detectados en el funcionamiento de nuestro codigo y generamos pruebas unitarias; ademas, modificamos el codigo para que usara vectores de Numpy en lugar de listas nativas de Python, evidenciando una clara mejora en los tiempos de ejecucion tomados. \n",
    "\n",
    "Uno de los desarrolladores Senior nos ha sugerido hacer pruebas de rendimiento usando el modulo Pandas. Para ello nos ha indicado que modifiquemos todas las partes del codigo que usan vectores de numpy por **Dataframes de Pandas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b53d424",
   "metadata": {},
   "source": [
    "# Insumos\n",
    "Tu equipo te ha dado el siguiente codigo el cual deberas modificar. \n",
    "\n",
    "**IMPORTANTE:** En la vida real lo comun es que pases mas tiempo leyendo codigo de otros, que escribiendolo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ad01e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import date\n",
    "from os.path import abspath\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NumpyLogExtractor:\n",
    "    \"\"\"Clase para extraer y filtrar logs desde un archivo JSON.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Inicializa el extractor de logs con la ruta del archivo.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Ruta del archivo JSON que contiene los logs.\n",
    "        \"\"\"\n",
    "        self.file_abspath = abspath(file_path)\n",
    "\n",
    "    def from_file(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Carga los logs desde el archivo JSON especificado en la inicialización.\n",
    "\n",
    "        Returns:\n",
    "            list[Log]: Lista de instancias de Log extraídas del archivo.\n",
    "        \"\"\"\n",
    "        log_dtype: list[tuple] = [\n",
    "            (\"sensor_id\", \"U16\"),\n",
    "            (\"date\", \"M8[D]\"),  # fecha con resolución de días\n",
    "            (\"event_type\", \"U16\"),\n",
    "            (\"duration_seconds\", \"f8\"),\n",
    "        ]\n",
    "\n",
    "        with open(self.file_abspath) as file:\n",
    "            raw_logs: list[dict] = json.load(file)\n",
    "        logs = np.array(\n",
    "            [\n",
    "                (\n",
    "                    log[\"sensor_id\"],\n",
    "                    np.datetime64(log[\"timestamp\"]),\n",
    "                    log[\"event_type\"],\n",
    "                    float(log[\"duration_seconds\"]),\n",
    "                )\n",
    "                for log in raw_logs\n",
    "            ],\n",
    "            dtype=log_dtype,\n",
    "        )\n",
    "        return logs\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_logs_on_date(\n",
    "        target_date: date, logs: np.ndarray, sensor_id: int | None = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Filtra los logs que corresponden a una fecha y, opcionalmente, a un sensor específico.\n",
    "\n",
    "        Args:\n",
    "            target_date (date): Fecha objetivo para filtrar los logs.\n",
    "            logs (list[Log]): Lista de logs disponibles.\n",
    "            sensor_id (Optional[int], optional): ID del sensor para filtrar.\n",
    "                Si es None, se incluyen logs de todos los sensores. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            list[Log]: Lista de logs que cumplen con los filtros aplicados.\n",
    "        \"\"\"\n",
    "        mask = logs[\"date\"] == np.datetime64(target_date)\n",
    "        if sensor_id is not None:\n",
    "            mask &= logs[\"sensor_id\"] == sensor_id\n",
    "        return logs[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78e7663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NumpySensorFailureAnalyzer:\n",
    "    \"\"\"Clase para analizar fallos de sensores a partir de registros de eventos usando NumPy.\"\"\"\n",
    "\n",
    "    WEEK_TOTAL_SECONDS: int = 7 * 24 * 60 * 60\n",
    "    \"\"\"Número total de segundos en una semana.\"\"\"\n",
    "\n",
    "    def __init__(self, logs: np.ndarray):\n",
    "        \"\"\"Inicializa el analizador con los registros de logs.\n",
    "\n",
    "        Args:\n",
    "            logs (np.ndarray): Array estructurado de logs con dtype:\n",
    "                [\n",
    "                    (\"sensor_id\", \"i4\"),\n",
    "                    (\"date\", \"M8[D]\"),\n",
    "                    (\"event_type\", \"U16\"),\n",
    "                    (\"duration_seconds\", \"f8\"),\n",
    "                ]\n",
    "        \"\"\"\n",
    "        self.logs = logs\n",
    "\n",
    "    def get_sensor_failure_duration(self, sensor_id: int, searched_date: date) -> float | None:\n",
    "        \"\"\"Obtiene la duración total del fallo de un sensor en una fecha específica.\"\"\"\n",
    "        mask = (self.logs[\"sensor_id\"] == sensor_id) & (\n",
    "            self.logs[\"date\"].astype(\"datetime64[D]\") == np.datetime64(searched_date)\n",
    "        )\n",
    "        logs_on_date = self.logs[mask]\n",
    "\n",
    "        if logs_on_date.size == 0:\n",
    "            return None\n",
    "\n",
    "        failure_start = logs_on_date[logs_on_date[\"event_type\"] == \"FAILURE_START\"][\n",
    "            \"duration_seconds\"\n",
    "        ]\n",
    "        failure_end = logs_on_date[logs_on_date[\"event_type\"] == \"FAILURE_END\"][\"duration_seconds\"]\n",
    "\n",
    "        start_time = failure_start[0] if failure_start.size > 0 else 0.0\n",
    "        end_time = failure_end[0] if failure_end.size > 0 else 0.0\n",
    "\n",
    "        return abs(end_time - start_time)\n",
    "\n",
    "    def get_weekly_sensor_failure_duration(\n",
    "        self, sensor_id: int, start_date: date, end_date: date | None = None\n",
    "    ) -> list[dict]:\n",
    "        \"\"\"Obtiene la duración diaria de fallos de un sensor durante una semana.\"\"\"\n",
    "        end_date = start_date + timedelta(7) if end_date is None else end_date\n",
    "        searched_week = [\n",
    "            start_date + timedelta(days=i) for i in range((end_date - start_date).days)\n",
    "        ]\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"sensor_id\": sensor_id,\n",
    "                \"date\": str(day),\n",
    "                \"failure_seconds\": self.get_sensor_failure_duration(sensor_id, day),\n",
    "            }\n",
    "            for day in searched_week\n",
    "        ]\n",
    "\n",
    "    def get_weekly_sensor_failure_probability(self, sensor_id: int, start_date: date) -> list[dict]:\n",
    "        \"\"\"Calcula la probabilidad diaria de fallo de un sensor durante una semana.\"\"\"\n",
    "        weekly_durations = self.get_weekly_sensor_failure_duration(sensor_id, start_date)\n",
    "        return [\n",
    "            {\n",
    "                \"sensor_id\": sensor_id,\n",
    "                \"date\": x[\"date\"],\n",
    "                \"failure_probability\": f\"{(x['failure_seconds'] or 0) / self.WEEK_TOTAL_SECONDS:.4f}\",\n",
    "            }\n",
    "            for x in weekly_durations\n",
    "        ]\n",
    "\n",
    "    def get_conditional_failure_probability(\n",
    "        self,\n",
    "        sensor_a_id: int,\n",
    "        sensor_b_id: int,\n",
    "        start_date: date,\n",
    "        failure_minutes_threshold: float | None = 3,\n",
    "    ) -> float:\n",
    "        \"\"\"Calcula la probabilidad condicional de que un sensor falle dado que otro sensor falló.\"\"\"\n",
    "        threshold = failure_minutes_threshold * 60\n",
    "\n",
    "        sensor_b_week = self.get_weekly_sensor_failure_duration(sensor_b_id, start_date)\n",
    "        count_b_failures = sum(\n",
    "            1 for x in sensor_b_week if x[\"failure_seconds\"] and x[\"failure_seconds\"] >= threshold\n",
    "        )\n",
    "        if count_b_failures == 0:\n",
    "            return 0.0\n",
    "\n",
    "        sensor_a_week = self.get_weekly_sensor_failure_duration(sensor_a_id, start_date)\n",
    "        count_a_b_failures = sum(\n",
    "            1\n",
    "            for x, y in zip(sensor_a_week, sensor_b_week, strict=False)\n",
    "            if x[\"failure_seconds\"]\n",
    "            and y[\"failure_seconds\"]\n",
    "            and x[\"failure_seconds\"] >= threshold\n",
    "            and y[\"failure_seconds\"] >= threshold\n",
    "        )\n",
    "\n",
    "        return count_a_b_failures / count_b_failures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d057a94",
   "metadata": {},
   "source": [
    "# Tu turno!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda8c8d",
   "metadata": {},
   "source": [
    "Ahora es tu turno! Evaluemos el rendimiento de nuestro codigo actual contra la modificacion sugerida por nuestro compañero de equipo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b9b956",
   "metadata": {},
   "source": [
    "## 1. Benchmarking de LogExtractor\n",
    "Reescribamos, probemos y comparemos los tiempos de ejecucion de la clase `NumpyLogExtractor` que tenemos en este momento, contra una version de esta clase modificada para usar Dataframes de Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79073d90",
   "metadata": {},
   "source": [
    "**Reescribiendo la clase NumpyLogExtractor:** Reescribe la clase NumpyLogExtractor para que use Dataframes de Pandas en lugar de vectores de Numpy.\n",
    "1. Nombra la nueva clase PandasLogExtractor. \n",
    "2. Modifica las funciones de esta clase que retornan `numpy.ndarray` para que retornen `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b0e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class PandasLogExtractor:\n",
    "    \"\"\"Clase para extraer y filtrar logs desde un archivo JSON usando pandas.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Inicializa el extractor de logs con la ruta del archivo.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Ruta del archivo JSON que contiene los logs.\n",
    "        \"\"\"\n",
    "        self.file_abspath = abspath(file_path)\n",
    "\n",
    "    def from_file(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Carga los logs desde el archivo JSON especificado en la inicialización.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame con las columnas:\n",
    "                - sensor_id (str)\n",
    "                - date (datetime64[ns])\n",
    "                - event_type (str)\n",
    "                - duration_seconds (float)\n",
    "        \"\"\"\n",
    "        with open(self.file_abspath) as file:\n",
    "            raw_logs: list[dict] = json.load(file)\n",
    "\n",
    "        logs_df = pd.DataFrame(raw_logs)\n",
    "\n",
    "        # Normalizar tipos de datos\n",
    "        logs_df[\"sensor_id\"] = logs_df[\"sensor_id\"].astype(str)\n",
    "        logs_df[\"date\"] = pd.to_datetime(logs_df[\"timestamp\"]).dt.date\n",
    "        logs_df[\"event_type\"] = logs_df[\"event_type\"].astype(str)\n",
    "        logs_df[\"duration_seconds\"] = logs_df[\"duration_seconds\"].astype(float)\n",
    "\n",
    "        return logs_df[[\"sensor_id\", \"date\", \"event_type\", \"duration_seconds\"]]\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_logs_on_date(\n",
    "        target_date: date, logs: pd.DataFrame, sensor_id: str | None = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filtra los logs que corresponden a una fecha y, opcionalmente, a un sensor específico.\n",
    "\n",
    "        Args:\n",
    "            target_date (date): Fecha objetivo para filtrar los logs.\n",
    "            logs (pd.DataFrame): DataFrame de logs disponibles.\n",
    "            sensor_id (Optional[str], optional): ID del sensor para filtrar.\n",
    "                Si es None, se incluyen logs de todos los sensores. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame con los logs que cumplen con los filtros aplicados.\n",
    "        \"\"\"\n",
    "        mask = logs[\"date\"] == target_date\n",
    "        if sensor_id is not None:\n",
    "            mask &= logs[\"sensor_id\"] == str(sensor_id)\n",
    "        return logs.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec474b0",
   "metadata": {},
   "source": [
    "Comparemos el tiempo de ejecucion de nuestra clase NumpyLogExtractor pasada contra esta nueva implementacion en Pandas para la funcion `fetch_logs_on_date`.\n",
    "- Consejo 1: Extrae los logs con ambas clases en una celda diferente a donde vayas a ejecutar `fetch_logs_on_date`; de no hacerlo, la medida del tiempo de ejecucion se vera contaminada con el tiempo tomado por la funcion de extraccion de logs.\n",
    "- Consejo 2: Busca sobre magics proporcionadas por Jupyter para medicion de tiempos y como usarlas. Consulta sobre `%%timeit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1fb86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "searched_date = datetime.strptime(\"2025-08-01\", \"%Y-%m-%d\").date()\n",
    "\n",
    "numpy_extracted_logs = NumpyLogExtractor(r\"../data/logs.json\").from_file()\n",
    "pandas_extracted_logs = PandasLogExtractor(r\"../data/logs.json\").from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e058bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.78 μs ± 55.9 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "NumpyLogExtractor.fetch_logs_on_date(searched_date, numpy_extracted_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ae386d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.4 μs ± 844 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "PandasLogExtractor.fetch_logs_on_date(searched_date, pandas_extracted_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4183672",
   "metadata": {},
   "source": [
    "## 2. Benchmarking de SensorFailureAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2884e194",
   "metadata": {},
   "source": [
    "Reescribamos, probemos y comparemos los tiempos de ejecucion de la clase `NumpySensorFailureAnalyzer` que tenemos en este momento, contra una version de esta clase modificada para usar dataframes de Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4872eaaf",
   "metadata": {},
   "source": [
    "**Reescribiendo la clase SensorFailureAnalyzer:** Reescribe la clase NumpySensorFailureAnalyzer para que use Datafarmes de Pandas en lugar de vectores de Numpy\n",
    "1. Nombra la nueva clase PandasSensorFailureAnalyzer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e150e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class PandasSensorFailureAnalyzer:\n",
    "    \"\"\"Clase para analizar fallos de sensores a partir de registros de eventos usando pandas.\"\"\"\n",
    "\n",
    "    WEEK_TOTAL_SECONDS: int = 7 * 24 * 60 * 60\n",
    "    \"\"\"Número total de segundos en una semana.\"\"\"\n",
    "\n",
    "    def __init__(self, logs: pd.DataFrame):\n",
    "        \"\"\"Inicializa el analizador con los registros de logs.\n",
    "\n",
    "        Args:\n",
    "            logs (pd.DataFrame): DataFrame con columnas:\n",
    "                - sensor_id (str)\n",
    "                - date (datetime.date)\n",
    "                - event_type (str)\n",
    "                - duration_seconds (float)\n",
    "        \"\"\"\n",
    "        self.logs = logs\n",
    "\n",
    "    def get_sensor_failure_duration(self, sensor_id: str, searched_date: date) -> float | None:\n",
    "        \"\"\"Obtiene la duración total del fallo de un sensor en una fecha específica.\"\"\"\n",
    "        logs_on_date = self.logs[\n",
    "            (self.logs[\"sensor_id\"] == str(sensor_id)) & (self.logs[\"date\"] == searched_date)\n",
    "        ]\n",
    "\n",
    "        if logs_on_date.empty:\n",
    "            return None\n",
    "\n",
    "        failure_start = logs_on_date.loc[\n",
    "            logs_on_date[\"event_type\"] == \"FAILURE_START\", \"duration_seconds\"\n",
    "        ]\n",
    "        failure_end = logs_on_date.loc[\n",
    "            logs_on_date[\"event_type\"] == \"FAILURE_END\", \"duration_seconds\"\n",
    "        ]\n",
    "\n",
    "        start_time = failure_start.iloc[0] if not failure_start.empty else 0.0\n",
    "        end_time = failure_end.iloc[0] if not failure_end.empty else 0.0\n",
    "\n",
    "        return abs(end_time - start_time)\n",
    "\n",
    "    def get_weekly_sensor_failure_duration(\n",
    "        self, sensor_id: str, start_date: date, end_date: date | None = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Obtiene la duración diaria de fallos de un sensor durante una semana.\"\"\"\n",
    "        end_date = start_date + timedelta(7) if end_date is None else end_date\n",
    "        searched_week = [\n",
    "            start_date + timedelta(days=i) for i in range((end_date - start_date).days)\n",
    "        ]\n",
    "\n",
    "        records = [\n",
    "            {\n",
    "                \"sensor_id\": sensor_id,\n",
    "                \"date\": day,\n",
    "                \"failure_seconds\": self.get_sensor_failure_duration(sensor_id, day),\n",
    "            }\n",
    "            for day in searched_week\n",
    "        ]\n",
    "\n",
    "        return pd.DataFrame(records)\n",
    "\n",
    "    def get_weekly_sensor_failure_probability(\n",
    "        self, sensor_id: str, start_date: date\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Calcula la probabilidad diaria de fallo de un sensor durante una semana.\"\"\"\n",
    "        weekly_durations = self.get_weekly_sensor_failure_duration(sensor_id, start_date)\n",
    "        weekly_durations[\"failure_probability\"] = (\n",
    "            weekly_durations[\"failure_seconds\"].fillna(0) / self.WEEK_TOTAL_SECONDS\n",
    "        ).round(4)\n",
    "        return weekly_durations[[\"sensor_id\", \"date\", \"failure_probability\"]]\n",
    "\n",
    "    def get_conditional_failure_probability(\n",
    "        self,\n",
    "        sensor_a_id: str,\n",
    "        sensor_b_id: str,\n",
    "        start_date: date,\n",
    "        failure_minutes_threshold: float | None = 3,\n",
    "    ) -> float:\n",
    "        \"\"\"Calcula la probabilidad condicional de que un sensor falle dado que otro sensor falló.\"\"\"\n",
    "        threshold = failure_minutes_threshold * 60\n",
    "\n",
    "        df_a = self.get_weekly_sensor_failure_duration(sensor_a_id, start_date)\n",
    "        df_b = self.get_weekly_sensor_failure_duration(sensor_b_id, start_date)\n",
    "\n",
    "        # Contar días con fallos\n",
    "        b_failures = df_b[\"failure_seconds\"].fillna(0) >= threshold\n",
    "        count_b_failures = b_failures.sum()\n",
    "\n",
    "        if count_b_failures == 0:\n",
    "            return 0.0\n",
    "\n",
    "        a_failures = df_a[\"failure_seconds\"].fillna(0) >= threshold\n",
    "        a_and_b_failures = a_failures & b_failures\n",
    "\n",
    "        return a_and_b_failures.sum() / count_b_failures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b172a15",
   "metadata": {},
   "source": [
    "Comparemos el tiempo de ejecucion de nuestra clase SensorFailureAnalyzer pasada contra esta nueva implementacion en Numpy para la funcion `get_weekly_sensor_failure_probability`.\n",
    "- Consejo 1: Usa los logs anteriormente extraidos e instancia (crea) ambas clases en una celda diferente a donde vayas a ejecutar `get_weekly_sensor_failure_probability`; de no hacerlo, la medida del tiempo de ejecucion se vera contaminada con el tiempo tomado para crear las clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47d7c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_analyzer = NumpySensorFailureAnalyzer(numpy_extracted_logs)\n",
    "pandas_analyzer = PandasSensorFailureAnalyzer(pandas_extracted_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af402dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.2 μs ± 185 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "numpy_analyzer.get_weekly_sensor_failure_probability(\n",
    "    sensor_id=\"thermo_core\", start_date=searched_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74443023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.65 ms ± 12.9 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pandas_analyzer.get_weekly_sensor_failure_probability(\n",
    "    sensor_id=\"thermo_core\", start_date=searched_date\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HenryLectures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
